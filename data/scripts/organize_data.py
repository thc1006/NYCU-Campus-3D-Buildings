#!/usr/bin/env python3
"""
NQSD æ•¸æ“šæ•´ç†å·¥å…·
åƒè€ƒ ymmap_archive çš„æ­¸æª”æ¨¡å¼é€²è¡Œæ•¸æ“šæ•´ç†
"""

import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from rich.console import Console
from rich.progress import track
from rich.table import Table

console = Console()

class DataOrganizer:
    """æ•¸æ“šæ•´ç†å™¨"""

    def __init__(self, base_path: Path):
        self.base_path = base_path
        self.raw_path = base_path / "raw"
        self.processed_path = base_path / "processed"
        self.output_path = base_path / "output"
        self.floor_plans_path = base_path / "floor_plans"
        self.backup_path = base_path / "backup"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def create_backup(self) -> Path:
        """å‰µå»ºå‚™ä»½"""
        console.print("\n[bold blue]ğŸ“¦ å‰µå»ºå‚™ä»½...[/bold blue]")

        backup_dir = self.backup_path / f"backup_{self.timestamp}"
        backup_dir.mkdir(parents=True, exist_ok=True)

        # å‚™ä»½å„å€‹ç›®éŒ„
        dirs_to_backup = ["raw", "processed", "output", "floor_plans"]

        for dir_name in track(dirs_to_backup, description="å‚™ä»½ä¸­"):
            src = self.base_path / dir_name
            if src.exists():
                dst = backup_dir / dir_name
                shutil.copytree(src, dst, dirs_exist_ok=True)

        console.print(f"[green]âœ“ å‚™ä»½å®Œæˆ: {backup_dir}[/green]")
        return backup_dir

    def organize_raw(self):
        """æ•´ç† raw/ ç›®éŒ„ - åŸºæ–¼ ymmap_archive æœ€ä½³å¯¦è¸"""
        console.print("\n[bold blue]ğŸ“ æ•´ç† raw/ ç›®éŒ„ï¼ˆåƒè€ƒ ymmap_archive æ¨¡å¼ï¼‰...[/bold blue]")

        # å‰µå»ºç‰ˆæœ¬åŒ–ç›®éŒ„çµæ§‹ï¼ˆåƒè€ƒ ymmap_archiveï¼‰
        tiles_dir = self.raw_path / "NLSC_3D_tiles"
        quadtree_dir = self.raw_path / "NLSC_quadtree"
        auxiliary_dir = self.raw_path / "auxiliary"

        for d in [tiles_dir / "current", tiles_dir / "previous", tiles_dir / "archive",
                  quadtree_dir / "current", quadtree_dir / "legacy", auxiliary_dir]:
            d.mkdir(parents=True, exist_ok=True)

        # ç‰ˆæœ¬æ˜ å°„ï¼ˆåŸºæ–¼ä»£ç†åˆ†æï¼‰
        version_mapping = {
            "current": ["113_J", "113_A", "112_A", "112_D", "112_O"],
            "previous": ["111_A", "111_J"],
            "legacy": ["109_A"]
        }

        # è™•ç† 3D Tiles
        tiles_datasets = list(self.raw_path.glob("NLSC_3D_tiles_*"))
        tiles_metadata = []

        for dataset in track(tiles_datasets, description="æ•´ç† 3D Tiles"):
            if not dataset.is_dir():
                continue

            # æå–ç‰ˆæœ¬è³‡è¨Š
            parts = dataset.name.replace("NLSC_3D_tiles_", "").split("_")
            year = parts[0] if parts else "unknown"
            campus_code = "_".join(parts[1:]) if len(parts) > 1 else "unknown"

            # ç¢ºå®šç‰ˆæœ¬åˆ†é¡
            version_category = "previous"  # é è¨­
            for category, patterns in version_mapping.items():
                if any(year.startswith(p) or campus_code.startswith(p) for p in patterns):
                    version_category = category
                    break

            # ç”Ÿæˆå…ƒæ•¸æ“š
            metadata = self._generate_dataset_metadata(dataset, year)
            metadata["version_category"] = version_category
            metadata["campus"] = self._identify_campus(campus_code)
            tiles_metadata.append(metadata)

            # ç§»å‹•åˆ°é©ç•¶çš„ç‰ˆæœ¬ç›®éŒ„
            new_name = f"{year}_{campus_code}"
            target_dir = tiles_dir / version_category / new_name

            if not target_dir.exists() and target_dir != dataset:
                shutil.move(str(dataset), str(target_dir))
                console.print(f"  ç§»å‹•: {dataset.name} â†’ {version_category}/{new_name}")

        # è™•ç† Quadtree
        quadtree_datasets = list(self.raw_path.glob("NLSC_quadtree_*"))
        quadtree_metadata = []

        for dataset in track(quadtree_datasets, description="æ•´ç† Quadtree"):
            if not dataset.is_dir():
                continue

            parts = dataset.name.replace("NLSC_quadtree_", "").split("_")
            year = parts[0] if parts else "unknown"
            campus_code = "_".join(parts[1:]) if len(parts) > 1 else "unknown"

            # v4 ç‰ˆæœ¬ç‰¹æ®Šè™•ç†
            if "v4" in campus_code.lower():
                version_category = "legacy"
            else:
                version_category = "current" if year.startswith("113") or year.startswith("112") else "legacy"

            metadata = self._generate_dataset_metadata(dataset, year)
            metadata["version_category"] = version_category
            metadata["campus"] = self._identify_campus(campus_code)
            quadtree_metadata.append(metadata)

            new_name = f"{year}_{campus_code}"
            target_dir = quadtree_dir / version_category / new_name

            if not target_dir.exists() and target_dir != dataset:
                shutil.move(str(dataset), str(target_dir))
                console.print(f"  ç§»å‹•: {dataset.name} â†’ {version_category}/{new_name}")

        # ç§»å‹•å¤–éƒ¨æ•¸æ“šåˆ° auxiliary/
        osm_files = list(self.raw_path.glob("taiwan-osm-*.*"))
        for osm_file in osm_files:
            target = auxiliary_dir / osm_file.name
            if not target.exists():
                shutil.move(str(osm_file), str(target))
                console.print(f"  ç§»å‹•: {osm_file.name} â†’ auxiliary/")

        # ä¿å­˜å…ƒæ•¸æ“šï¼ˆåƒè€ƒ ymmap_archive çš„å…ƒæ•¸æ“šçµæ§‹ï¼‰
        self._save_raw_metadata(tiles_dir, tiles_metadata, "3D_tiles")
        self._save_raw_metadata(quadtree_dir, quadtree_metadata, "quadtree")

        # å‰µå»º README
        self._create_raw_readme()

        console.print("[green]âœ“ raw/ ç›®éŒ„æ•´ç†å®Œæˆï¼ˆå·²åˆ†é¡ï¼šcurrent/previous/legacyï¼‰[/green]")

    def _identify_campus(self, campus_code: str) -> str:
        """è­˜åˆ¥æ ¡å€åç¨±"""
        campus_map = {
            "yangming": "é™½æ˜",
            "A": "é™½æ˜",
            "boai": "åšæ„›",
            "O": "å…‰å¾©/åšæ„›",
            "gueiren": "æ­¸ä»",
            "D": "æ­¸ä»",
            "liujia": "å…­ç”²",
            "J": "å…­ç”²"
        }
        for key, value in campus_map.items():
            if key.lower() in campus_code.lower():
                return value
        return "æœªçŸ¥"

    def _save_raw_metadata(self, base_dir: Path, metadata_list: List[Dict], data_type: str):
        """ä¿å­˜ raw ç›®éŒ„å…ƒæ•¸æ“š"""
        metadata = {
            "description": f"NLSC {data_type} Data - Versioned Organization",
            "source": "National Land Surveying and Mapping Center (NLSC)",
            "coordinate_system": "TWD97 (EPSG:3826)",
            "organized_date": self.timestamp,
            "version_strategy": {
                "current": "Latest versions (113_*, 112_*)",
                "previous": "Previous versions (111_*)",
                "legacy": "Older versions (109_*, special v4)"
            },
            "datasets": metadata_list,
            "total_count": len(metadata_list),
            "total_size_mb": sum(d.get("total_size_mb", 0) for d in metadata_list)
        }

        metadata_file = base_dir / "metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    def organize_processed(self):
        """æ•´ç† processed/ ç›®éŒ„ - æŒ‰æ ¡å€å’Œæ•¸æ“šæºæ­£ç¢ºåˆ†é¡"""
        console.print("\n[bold blue]ğŸ“ æ•´ç† processed/ ç›®éŒ„ï¼ˆæŒ‰æ ¡å€å’Œæ•¸æ“šæºåˆ†é¡ï¼‰...[/bold blue]")

        # å‰µå»ºè©³ç´°çš„ç›®éŒ„çµæ§‹ï¼ˆåƒè€ƒ ymmap_archive çš„åˆ†é¡æ–¹å¼ï¼‰
        buildings_dir = self.processed_path / "buildings"
        reference_dir = self.processed_path / "reference"

        # ç‚ºæ¯å€‹æ ¡å€å‰µå»ºç›®éŒ„
        campus_list = ["boai", "yangming", "gueiren", "liujia", "guangfu"]
        for campus in campus_list:
            (buildings_dir / "by_campus" / campus).mkdir(parents=True, exist_ok=True)

        (buildings_dir / "combined").mkdir(parents=True, exist_ok=True)
        (buildings_dir / "osm").mkdir(parents=True, exist_ok=True)
        reference_dir.mkdir(parents=True, exist_ok=True)

        # æ ¡å€æ–‡ä»¶æ˜ å°„ï¼ˆåƒè€ƒä»£ç†åˆ†æçµæœï¼‰
        campus_mapping = {
            "boai": {
                "json": "NYCU_boai_NLSC_buildings.json",
                "building_count": 1023,
                "data_source": "NLSC Layer 112_O"
            },
            "yangming": {
                "json": "NYCU_yangming_NLSC_buildings.json",
                "building_count": 446,
                "data_source": "NLSC Layer 112_A/113_A"
            },
            "gueiren": {
                "json": "NYCU_gueiren_NLSC_buildings.json",
                "building_count": 17,
                "data_source": "NLSC Layer 112_D"
            },
            "liujia": {
                "json": "NYCU_liujia_NLSC_buildings.json",
                "building_count": 169,
                "data_source": "NLSC Layer 113_J"
            },
            "guangfu": {
                "geojson": "NYCU_Guangfu_OSM_buildings.geojson",
                "building_count": 319,
                "data_source": "OpenStreetMap"
            }
        }

        # è™•ç†æ ¡å€æ–‡ä»¶
        for campus, info in track(campus_mapping.items(), description="åˆ†é¡æ ¡å€æ•¸æ“š"):
            campus_dir = buildings_dir / "by_campus" / campus

            # ç§»å‹•ä¸»æ–‡ä»¶
            for file_type in ["json", "geojson"]:
                if file_type in info:
                    src_file = self.processed_path / info[file_type]
                    if src_file.exists():
                        # çµ±ä¸€å‘½åæ ¼å¼ï¼ˆå»æ‰ NYCU_ å‰ç¶´ï¼‰
                        if campus == "guangfu":
                            dst_name = "OSM_buildings.geojson"
                        else:
                            dst_name = f"NLSC_buildings.{file_type}"

                        dst_path = campus_dir / dst_name
                        if not dst_path.exists():
                            shutil.copy2(src_file, dst_path)
                            console.print(f"  âœ“ {campus}: {info[file_type]} â†’ {dst_name}")

            # ç‚ºæ¯å€‹æ ¡å€ç”Ÿæˆå…ƒæ•¸æ“š
            campus_metadata = {
                "campus": campus,
                "campus_name_zh": self._get_campus_name_zh(campus),
                "campus_name_en": self._get_campus_name_en(campus),
                "data_source": info["data_source"],
                "building_count": info["building_count"],
                "files": []
            }

            # æª¢æŸ¥å¯¦éš›æ–‡ä»¶
            for f in campus_dir.glob("*.*"):
                if f.is_file():
                    campus_metadata["files"].append({
                        "filename": f.name,
                        "size_bytes": f.stat().st_size,
                        "format": f.suffix[1:]
                    })

            # ä¿å­˜æ ¡å€å…ƒæ•¸æ“š
            with open(campus_dir / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(campus_metadata, f, indent=2, ensure_ascii=False)

        # è™•ç†åˆä½µæ–‡ä»¶
        combined_mapping = {
            "NYCU_NLSC_buildings.json": {
                "new_name": "with_surrounding.json",
                "description": "åŒ…å«å‘¨é‚Šå»ºç‰©çš„å®Œæ•´æ•¸æ“šï¼ˆ6,181 æ£Ÿï¼‰"
            },
            "NYCU_NLSC_buildings.geojson": {
                "new_name": "with_surrounding.geojson",
                "description": "GeoJSON æ ¼å¼ï¼ˆ6,181 æ£Ÿï¼‰"
            }
        }

        console.print("\n  è™•ç†åˆä½µæ•¸æ“š...")
        for src_name, info in combined_mapping.items():
            src_file = self.processed_path / src_name
            if src_file.exists():
                dst_path = buildings_dir / "combined" / info["new_name"]
                if not dst_path.exists():
                    shutil.copy2(src_file, dst_path)
                    console.print(f"  âœ“ åˆä½µ: {src_name} â†’ {info['new_name']}")

        # ç§»å‹•åƒè€ƒæ–‡ä»¶
        building_list = self.processed_path / "NYCU_building_list.txt"
        if building_list.exists():
            dst = reference_dir / "building_names_list.txt"
            if not dst.exists():
                shutil.copy2(building_list, dst)
                console.print(f"  âœ“ åƒè€ƒ: NYCU_building_list.txt â†’ reference/")

        # ç”Ÿæˆç¸½é«”å…ƒæ•¸æ“š
        overall_metadata = {
            "organized_date": self.timestamp,
            "organization_principle": "ymmap_archive style - hierarchical by campus and source",
            "structure": {
                "buildings/by_campus/{campus}/": "å„æ ¡å€çš„ç¨ç«‹æ•¸æ“š",
                "buildings/combined/": "æ‰€æœ‰æ ¡å€åˆä½µæ•¸æ“šï¼ˆå«å‘¨é‚Šå»ºç‰©ï¼‰",
                "buildings/osm/": "OpenStreetMap ä¾†æºæ•¸æ“š",
                "reference/": "åƒè€ƒå’Œç´¢å¼•æ–‡ä»¶"
            },
            "campuses": list(campus_mapping.keys()),
            "total_buildings_by_campus": sum(info["building_count"] for info in campus_mapping.values()),
            "data_sources": {
                "NLSC": "4 campuses (boai, yangming, gueiren, liujia)",
                "OSM": "1 campus (guangfu)"
            }
        }

        with open(self.processed_path / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(overall_metadata, f, indent=2, ensure_ascii=False)

        self._create_processed_readme()

        console.print("[green]âœ“ processed/ ç›®éŒ„æ•´ç†å®Œæˆï¼ˆ5 å€‹æ ¡å€å·²åˆ†é¡ï¼‰[/green]")

    def _get_campus_name_zh(self, campus: str) -> str:
        """ç²å–æ ¡å€ä¸­æ–‡åç¨±"""
        names = {
            "boai": "åšæ„›æ ¡å€",
            "yangming": "é™½æ˜æ ¡å€",
            "gueiren": "æ­¸ä»æ ¡å€",
            "liujia": "å…­ç”²æ ¡å€",
            "guangfu": "å…‰å¾©æ ¡å€"
        }
        return names.get(campus, campus)

    def _get_campus_name_en(self, campus: str) -> str:
        """ç²å–æ ¡å€è‹±æ–‡åç¨±"""
        names = {
            "boai": "Boai Campus",
            "yangming": "Yangming Campus",
            "gueiren": "Gueiren Campus",
            "liujia": "Liujia Campus",
            "guangfu": "Guangfu Campus"
        }
        return names.get(campus, campus.title())

    def organize_output(self):
        """æ•´ç† output/ ç›®éŒ„ - å»ºç«‹ç‰ˆæœ¬æ§åˆ¶"""
        console.print("\n[bold blue]ğŸ“ æ•´ç† output/ ç›®éŒ„...[/bold blue]")

        # å‰µå»ºç‰ˆæœ¬åŒ–ç›®éŒ„
        version_name = f"v1_{datetime.now().strftime('%Y-%m-%d')}"
        version_dir = self.output_path / version_name
        version_dir.mkdir(exist_ok=True)

        latest_dir = self.output_path / "latest"

        # ç•¶å‰çš„è¼¸å‡ºæ–‡ä»¶
        output_files = [
            "NYCU_buildings_3d.geojson",
            "NYCU_buildings_3d.html",
            "NYCU_buildings_map.html",
            "NYCU_buildings_merged.geojson",
            "NYCU_buildings_table.csv",
            "NYCU_buildings_table.xlsx",
        ]

        # è¤‡è£½åˆ°ç‰ˆæœ¬ç›®éŒ„
        for filename in track(output_files, description="ç‰ˆæœ¬åŒ–æ–‡ä»¶"):
            src = self.output_path / filename
            if src.exists():
                # å»æ‰ NYCU_ å‰ç¶´
                new_name = filename.replace("NYCU_", "")
                dst = version_dir / new_name
                shutil.copy2(src, dst)

        # ç”Ÿæˆç‰ˆæœ¬å…ƒæ•¸æ“š
        metadata = {
            "version": version_name,
            "created_date": datetime.now().isoformat(),
            "files": output_files,
            "source": "processed/buildings/combined/",
            "generator": "building_merger_v2.py"
        }

        with open(version_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # å‰µå»ºæˆ–æ›´æ–° latest/ ç¬¦è™Ÿé€£çµï¼ˆWindows ä½¿ç”¨ç›®éŒ„è¤‡è£½ï¼‰
        if latest_dir.exists():
            shutil.rmtree(latest_dir)
        shutil.copytree(version_dir, latest_dir)

        self._create_output_readme()

        console.print("[green]âœ“ output/ ç›®éŒ„æ•´ç†å®Œæˆ[/green]")

    def organize_floor_plans(self):
        """æ•´ç† floor_plans/ ç›®éŒ„ - æŒ‰é¡å‹åˆ†é¡ä¸¦ç”Ÿæˆè©³ç´°å…ƒæ•¸æ“š"""
        console.print("\n[bold blue]ğŸ“ æ•´ç† floor_plans/ ç›®éŒ„ï¼ˆæŒ‰é¡å‹åˆ†é¡ï¼‰...[/bold blue]")

        # å‰µå»ºè©³ç´°çš„ç›®éŒ„çµæ§‹
        pdf_dir = self.floor_plans_path / "pdf"
        categories = {
            "auditorium": {
                "label_zh": "ç¦®å ‚",
                "label_en": "Auditorium",
                "building": "Main Auditorium",
                "campus": "Yangming"
            },
            "buildings": {
                "label_zh": "å»ºç¯‰ç‰©",
                "label_en": "Buildings",
                "building": "Various",
                "campus": "Yangming"
            },
            "campus": {
                "label_zh": "æ ¡åœ’åœ°åœ–",
                "label_en": "Campus Maps",
                "building": "Campus-wide",
                "campus": "Yangming"
            },
            "administrative": {
                "label_zh": "è¡Œæ”¿æ–‡ä»¶",
                "label_en": "Administrative",
                "building": "Administrative",
                "campus": "All"
            }
        }

        for category in categories:
            (pdf_dir / category).mkdir(parents=True, exist_ok=True)

        # è©³ç´°çš„æ–‡ä»¶æ˜ å°„ï¼ˆåŸºæ–¼ä»£ç†åˆ†æï¼‰
        pdf_mapping = {
            "auditorium_panorama.pdf": {
                "category": "auditorium",
                "new_name": "panorama.pdf",
                "title_zh": "ç¦®å ‚å…¨æ™¯åœ–",
                "title_en": "Auditorium Panorama",
                "pages": 2,
                "use_cases": ["event_planning", "capacity_planning"]
            },
            "auditorium_seatmap.pdf": {
                "category": "auditorium",
                "new_name": "seatmap.pdf",
                "title_zh": "ç¦®å ‚åº§ä½åœ–",
                "title_en": "Auditorium Seat Map",
                "pages": 2,
                "use_cases": ["seating_assignment", "ticket_planning"]
            },
            "einfo_building_map.pdf": {
                "category": "buildings",
                "new_name": "einfo_building_map.pdf",
                "title_zh": "è³‡è¨Šå¤§æ¨“å¹³é¢åœ–",
                "title_en": "E-Info Building Map",
                "pages": 2,
                "use_cases": ["navigation", "room_finding"]
            },
            "eng5_exam_floorplan.pdf": {
                "category": "buildings",
                "new_name": "eng5_exam_floorplan.pdf",
                "title_zh": "å·¥ç¨‹äº”é¤¨è€ƒè©¦é…ç½®åœ–",
                "title_en": "ENG5 Exam Floor Plan",
                "pages": 1,
                "use_cases": ["exam_planning", "seating_arrangement"]
            },
            "yangming_campus_map.pdf": {
                "category": "campus",
                "new_name": "yangming_campus_map.pdf",
                "title_zh": "é™½æ˜æ ¡å€åœ°åœ–",
                "title_en": "Yangming Campus Map",
                "pages": 1,
                "use_cases": ["navigation", "wayfinding"]
            },
            "yangming_map_old.pdf": {
                "category": "campus",
                "new_name": "yangming_map_old.pdf",
                "title_zh": "é™½æ˜æ ¡å€åœ°åœ–ï¼ˆèˆŠç‰ˆï¼‰",
                "title_en": "Yangming Campus Map (Old)",
                "pages": 2,
                "use_cases": ["historical_reference"]
            },
            "fee_standard.pdf": {
                "category": "administrative",
                "new_name": "fee_standard.pdf",
                "title_zh": "æ”¶è²»æ¨™æº–",
                "title_en": "Fee Standard",
                "pages": 0,  # æå£
                "status": "corrupted",
                "use_cases": ["fee_reference"]
            }
        }

        # ç§»å‹•å’Œåˆ†é¡ PDF æ–‡ä»¶
        file_stats = {}
        for src_name, info in track(pdf_mapping.items(), description="åˆ†é¡ PDF æ–‡ä»¶"):
            src_file = self.floor_plans_path / src_name
            category = info["category"]
            dst_path = pdf_dir / category / info["new_name"]

            if src_file.exists() and not dst_path.exists():
                # æª¢æŸ¥æ–‡ä»¶å¤§å°ï¼ˆæª¢æ¸¬æå£ï¼‰
                file_size = src_file.stat().st_size
                if file_size < 1000:  # å°æ–¼ 1 KB å¯èƒ½æå£
                    console.print(f"  âš ï¸  {src_name}: æª”æ¡ˆå¯èƒ½æå£ï¼ˆ{file_size} bytesï¼‰")
                    info["status"] = "corrupted"
                    info["size_bytes"] = file_size

                shutil.copy2(src_file, dst_path)
                console.print(f"  âœ“ {category}: {src_name} â†’ {info['new_name']}")

            # çµ±è¨ˆ
            if category not in file_stats:
                file_stats[category] = {
                    "count": 0,
                    "total_size": 0,
                    "files": []
                }
            file_stats[category]["count"] += 1
            file_stats[category]["files"].append(info)
            if src_file.exists():
                file_stats[category]["total_size"] += src_file.stat().st_size

        # é‡çµ„ preview/ ç›®éŒ„ï¼ˆä¿æŒèˆ‡ PDF ä¸€è‡´çš„çµæ§‹ï¼‰
        console.print("\n  é‡çµ„ preview/ ç›®éŒ„...")
        preview_base = self.floor_plans_path / "preview"

        preview_mapping = {
            "auditorium": ["auditorium_panorama_", "auditorium_seatmap_"],
            "buildings": ["einfo_building_map_", "eng5_exam_floorplan_"],
            "campus": ["yangming_campus_map_", "yangming_map_old_"]
        }

        for category, prefixes in preview_mapping.items():
            category_preview_dir = preview_base / category
            category_preview_dir.mkdir(parents=True, exist_ok=True)

            for prefix in prefixes:
                for preview_file in preview_base.glob(f"{prefix}*.png"):
                    dst = category_preview_dir / preview_file.name
                    if not dst.exists():
                        shutil.copy2(preview_file, dst)

        # ç”Ÿæˆè©³ç´°çš„å…ƒæ•¸æ“š
        metadata = {
            "organized_date": self.timestamp,
            "organization_principle": "ymmap_archive style - hierarchical by document type",
            "categories": {}
        }

        for category, info in categories.items():
            stats = file_stats.get(category, {"count": 0, "files": []})

            # çµ±è¨ˆé è¦½åœ–
            preview_count = len(list((preview_base / category).glob("*.png"))) if (preview_base / category).exists() else 0

            metadata["categories"][category] = {
                "label_zh": info["label_zh"],
                "label_en": info["label_en"],
                "building": info["building"],
                "campus": info["campus"],
                "document_count": stats["count"],
                "preview_count": preview_count,
                "total_size_mb": round(stats.get("total_size", 0) / (1024 * 1024), 2),
                "documents": []
            }

            # æ·»åŠ æ–‡ä»¶è©³æƒ…
            for file_info in stats.get("files", []):
                doc_meta = {
                    "id": file_info["new_name"].replace(".pdf", ""),
                    "filename": file_info["new_name"],
                    "title_zh": file_info.get("title_zh", ""),
                    "title_en": file_info.get("title_en", ""),
                    "pages": file_info.get("pages", 0),
                    "use_cases": file_info.get("use_cases", [])
                }

                if "status" in file_info:
                    doc_meta["status"] = file_info["status"]
                    doc_meta["size_bytes"] = file_info.get("size_bytes", 0)

                metadata["categories"][category]["documents"].append(doc_meta)

        # æ·»åŠ å“è³ªå•é¡Œè¨˜éŒ„
        metadata["quality_issues"] = []
        for src_name, info in pdf_mapping.items():
            if info.get("status") == "corrupted":
                metadata["quality_issues"].append({
                    "document": src_name,
                    "category": info["category"],
                    "issue": f"File size {info.get('size_bytes', 0)} bytes - likely corrupted",
                    "action_required": "Replace with valid file or remove",
                    "priority": "high"
                })

        with open(self.floor_plans_path / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        self._create_floor_plans_readme()

        total_issues = len(metadata["quality_issues"])
        if total_issues > 0:
            console.print(f"[yellow]âš ï¸  ç™¼ç¾ {total_issues} å€‹å“è³ªå•é¡Œï¼ˆè©³è¦‹ metadata.jsonï¼‰[/yellow]")

        console.print("[green]âœ“ floor_plans/ ç›®éŒ„æ•´ç†å®Œæˆï¼ˆ4 å€‹é¡åˆ¥å·²åˆ†é¡ï¼‰[/green]")

    def _generate_dataset_metadata(self, dataset_path: Path, year: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ•¸æ“šé›†å…ƒæ•¸æ“š"""
        bin_files = list(dataset_path.rglob("*.bin"))
        layers = set([f.parent.name for f in bin_files if f.parent.name.startswith("L")])

        total_size = sum(f.stat().st_size for f in bin_files) / (1024 * 1024)  # MB

        return {
            "name": dataset_path.name,
            "year": year,
            "file_count": len(bin_files),
            "total_size_mb": round(total_size, 2),
            "layers": sorted(list(layers)),
            "has_manifest": (dataset_path / "manifest.json").exists()
        }

    def _create_raw_readme(self):
        """å‰µå»º raw/ ç›®éŒ„çš„ README"""
        readme_content = """# Raw Data - åŸå§‹æ•¸æ“š

æ­¤ç›®éŒ„åŒ…å«å¾ NLSCï¼ˆåœ‹åœŸæ¸¬ç¹ªä¸­å¿ƒï¼‰ä¸‹è¼‰çš„åŸå§‹ 3D Tiles æ•¸æ“šã€‚

## ç›®éŒ„çµæ§‹

```
raw/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ NLSC_3D_tiles/              # 3D Tiles æ•¸æ“šé›†
â”‚   â”œâ”€â”€ metadata.json           # æ•¸æ“šé›†å…ƒæ•¸æ“š
â”‚   â”œâ”€â”€ 109_A_yangming/         # 109 å¹´é™½æ˜æ ¡å€
â”‚   â”œâ”€â”€ 112_A_yangming/         # 112 å¹´é™½æ˜æ ¡å€
â”‚   â”œâ”€â”€ 112_D_gueiren/          # 112 å¹´æ­¸ä»æ ¡å€
â”‚   â”œâ”€â”€ 112_O/                  # 112 å¹´å…¶ä»–
â”‚   â”œâ”€â”€ 112_O_boai/             # 112 å¹´åšæ„›æ ¡å€
â”‚   â””â”€â”€ 113_J_liujia/           # 113 å¹´å…­ç”²æ ¡å€
â””â”€â”€ archive/                    # æ­¸æª”çš„èˆŠç‰ˆæœ¬ï¼ˆå£“ç¸®ï¼‰
```

## æ•¸æ“šä¾†æº

- **ä¾†æº**: åœ‹åœŸæ¸¬ç¹ªä¸­å¿ƒ (NLSC)
- **æ ¼å¼**: 3D Tiles (.bin)
- **åæ¨™ç³»çµ±**: TWD97 (EPSG:3826)

## ä½¿ç”¨èªªæ˜

1. æ¯å€‹æ•¸æ“šé›†åŒ…å«å¤šå€‹å±¤ç´šï¼ˆL5, L6 ç­‰ï¼‰çš„ 3D Tiles
2. `manifest.json` è¨˜éŒ„äº† tiles çš„ç´¢å¼•è³‡è¨Š
3. ä¸å»ºè­°ç›´æ¥ä¿®æ”¹æ­¤ç›®éŒ„ä¸‹çš„æ–‡ä»¶
4. æ•¸æ“šè™•ç†è«‹ä½¿ç”¨ `processed/` ç›®éŒ„çš„çµæœ

## æ›´æ–°è¨˜éŒ„

- 2026-02-08: æ•´ç†ä¸¦å»ºç«‹æ–°çš„ç›®éŒ„çµæ§‹
"""

        with open(self.raw_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)

    def _create_processed_readme(self):
        """å‰µå»º processed/ ç›®éŒ„çš„ README"""
        readme_content = """# Processed Data - è™•ç†å¾Œçš„æ•¸æ“š

æ­¤ç›®éŒ„åŒ…å«å¾åŸå§‹ 3D Tiles æå–ä¸¦è™•ç†å¾Œçš„å»ºç¯‰æ•¸æ“šã€‚

## ç›®éŒ„çµæ§‹

```
processed/
â”œâ”€â”€ README.md                           # æœ¬æ–‡ä»¶
â”œâ”€â”€ metadata.json                       # è™•ç†å…ƒæ•¸æ“š
â”œâ”€â”€ buildings/
â”‚   â”œâ”€â”€ by_campus/                      # æŒ‰æ ¡å€åˆ†é¡
â”‚   â”‚   â”œâ”€â”€ boai_NLSC_buildings.json
â”‚   â”‚   â”œâ”€â”€ gueiren_NLSC_buildings.json
â”‚   â”‚   â”œâ”€â”€ liujia_NLSC_buildings.json
â”‚   â”‚   â””â”€â”€ yangming_NLSC_buildings.json
â”‚   â”œâ”€â”€ combined/                       # åˆä½µæ•¸æ“š
â”‚   â”‚   â”œâ”€â”€ NLSC_buildings.json
â”‚   â”‚   â””â”€â”€ NLSC_buildings.geojson
â”‚   â””â”€â”€ osm/                            # OpenStreetMap æ•¸æ“š
â”‚       â””â”€â”€ Guangfu_OSM_buildings.geojson
â””â”€â”€ building_list.txt                   # å»ºç¯‰æ¸…å–®
```

## æ•¸æ“šæ ¼å¼

- **JSON**: åŸå§‹å»ºç¯‰è³‡è¨Šï¼ˆåŒ…å«é«˜åº¦ã€æ¨“å±¤ç­‰ï¼‰
- **GeoJSON**: åœ°ç†ç©ºé–“æ ¼å¼ï¼ˆå¯ç”¨æ–¼ GIS è»Ÿé«”ï¼‰

## è™•ç†æµç¨‹

1. å¾ raw/ è®€å– 3D Tiles
2. æå–å»ºç¯‰å¤šé‚Šå½¢
3. è¨ˆç®—å»ºç¯‰é«˜åº¦å’Œæ¨“å±¤
4. æŒ‰æ ¡å€åˆ†é¡
5. åˆä½µç‚ºå®Œæ•´æ•¸æ“šé›†

## ä½¿ç”¨ç¯„ä¾‹

```python
import json
import geopandas as gpd

# è®€å– GeoJSON
gdf = gpd.read_file("buildings/combined/NLSC_buildings.geojson")

# è®€å– JSON
with open("buildings/by_campus/yangming_NLSC_buildings.json") as f:
    data = json.load(f)
```
"""

        with open(self.processed_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)

    def _create_output_readme(self):
        """å‰µå»º output/ ç›®éŒ„çš„ README"""
        readme_content = """# Output - æœ€çµ‚è¼¸å‡º

æ­¤ç›®éŒ„åŒ…å«æœ€çµ‚çš„å¯è¦–åŒ–å’Œåˆ†æçµæœã€‚

## ç›®éŒ„çµæ§‹

```
output/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ latest/                      # æœ€æ–°ç‰ˆæœ¬ï¼ˆç¬¦è™Ÿé€£çµï¼‰
â”‚   â”œâ”€â”€ buildings_3d.geojson
â”‚   â”œâ”€â”€ buildings_3d.html
â”‚   â”œâ”€â”€ buildings_map.html
â”‚   â”œâ”€â”€ buildings_merged.geojson
â”‚   â”œâ”€â”€ buildings_table.csv
â”‚   â”œâ”€â”€ buildings_table.xlsx
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ v1_2026-02-07/              # ç‰ˆæœ¬åŒ–å­˜æª”
â”‚   â””â”€â”€ [same files as latest]
â””â”€â”€ archive/                     # èˆŠç‰ˆæœ¬ï¼ˆå£“ç¸®ï¼‰
```

## æ–‡ä»¶èªªæ˜

| æ–‡ä»¶ | æ ¼å¼ | æè¿° |
|------|------|------|
| `buildings_3d.geojson` | GeoJSON | 3D å»ºç¯‰æ•¸æ“š |
| `buildings_3d.html` | HTML | 3D å¯è¦–åŒ–åœ°åœ– |
| `buildings_map.html` | HTML | 2D äº’å‹•åœ°åœ– |
| `buildings_merged.geojson` | GeoJSON | åˆä½µçš„å®Œæ•´æ•¸æ“š |
| `buildings_table.csv` | CSV | å»ºç¯‰è³‡æ–™è¡¨ |
| `buildings_table.xlsx` | Excel | å»ºç¯‰è³‡æ–™è¡¨ï¼ˆå¸¶æ ¼å¼ï¼‰ |

## ç‰ˆæœ¬ç®¡ç†

- `latest/`: æ°¸é æŒ‡å‘æœ€æ–°ç‰ˆæœ¬
- `vX_YYYY-MM-DD/`: å¸¶æ™‚é–“æˆ³çš„ç‰ˆæœ¬å­˜æª”
- `archive/`: å£“ç¸®çš„èˆŠç‰ˆæœ¬ï¼ˆç¯€çœç©ºé–“ï¼‰

## ä½¿ç”¨èªªæ˜

1. **æŸ¥çœ‹åœ°åœ–**: ç›´æ¥é–‹å•Ÿ `latest/buildings_map.html`
2. **æ•¸æ“šåˆ†æ**: ä½¿ç”¨ CSV æˆ– GeoJSON æ–‡ä»¶
3. **ç‰ˆæœ¬è¿½æº¯**: æŸ¥çœ‹ç‰¹å®šæ—¥æœŸçš„ç‰ˆæœ¬ç›®éŒ„
"""

        with open(self.output_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)

    def _create_floor_plans_readme(self):
        """å‰µå»º floor_plans/ ç›®éŒ„çš„ README"""
        readme_content = """# Floor Plans - å¹³é¢åœ–

æ­¤ç›®éŒ„åŒ…å«æ ¡åœ’å»ºç¯‰çš„å¹³é¢åœ–å’Œç›¸é—œæ–‡ä»¶ã€‚

## ç›®éŒ„çµæ§‹

```
floor_plans/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ metadata.json                # å¹³é¢åœ–ç´¢å¼•
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ auditorium/              # ç¦®å ‚ç›¸é—œ
â”‚   â”‚   â”œâ”€â”€ panorama.pdf
â”‚   â”‚   â””â”€â”€ seatmap.pdf
â”‚   â”œâ”€â”€ buildings/               # å»ºç¯‰ç‰©å¹³é¢åœ–
â”‚   â”‚   â”œâ”€â”€ einfo_building_map.pdf
â”‚   â”‚   â””â”€â”€ eng5_exam_floorplan.pdf
â”‚   â”œâ”€â”€ campus/                  # æ ¡åœ’åœ°åœ–
â”‚   â”‚   â”œâ”€â”€ yangming_campus_map.pdf
â”‚   â”‚   â””â”€â”€ yangming_map_old.pdf
â”‚   â””â”€â”€ administrative/          # è¡Œæ”¿æ–‡ä»¶
â”‚       â””â”€â”€ fee_standard.pdf
â””â”€â”€ preview/                     # PNG é è¦½åœ–
    â”œâ”€â”€ auditorium/
    â”œâ”€â”€ buildings/
    â””â”€â”€ campus/
```

## æ–‡ä»¶åˆ†é¡

### ğŸ›ï¸ Auditorium (ç¦®å ‚)
- `panorama.pdf`: ç¦®å ‚å…¨æ™¯åœ–
- `seatmap.pdf`: åº§ä½é…ç½®åœ–

### ğŸ¢ Buildings (å»ºç¯‰ç‰©)
- `einfo_building_map.pdf`: è³‡è¨Šå¤§æ¨“å¹³é¢åœ–
- `eng5_exam_floorplan.pdf`: å·¥ç¨‹äº”é¤¨è€ƒå ´é…ç½®

### ğŸ—ºï¸ Campus (æ ¡åœ’)
- `yangming_campus_map.pdf`: é™½æ˜æ ¡å€åœ°åœ–ï¼ˆæ–°ï¼‰
- `yangming_map_old.pdf`: é™½æ˜æ ¡å€åœ°åœ–ï¼ˆèˆŠï¼‰

### ğŸ“‹ Administrative (è¡Œæ”¿)
- `fee_standard.pdf`: æ”¶è²»æ¨™æº–

## é è¦½åœ–

`preview/` ç›®éŒ„åŒ…å«æ‰€æœ‰ PDF çš„ PNG é è¦½åœ–ï¼ˆæ¯é ä¸€å¼µï¼‰ã€‚

## ä½¿ç”¨å»ºè­°

- éœ€è¦åˆ—å°æ™‚ä½¿ç”¨ PDF åŸæª”
- å¿«é€ŸæŸ¥çœ‹æ™‚ä½¿ç”¨ preview/ çš„ PNG åœ–ç‰‡
- å»ºè­°ä½¿ç”¨ PDF é–±è®€å™¨é–‹å•Ÿä»¥ç²å¾—æœ€ä½³é«”é©—
"""

        with open(self.floor_plans_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)

    def generate_report(self) -> str:
        """ç”Ÿæˆæ•´ç†å ±å‘Š"""
        console.print("\n[bold blue]ğŸ“Š ç”Ÿæˆæ•´ç†å ±å‘Š...[/bold blue]")

        # çµ±è¨ˆå„ç›®éŒ„
        stats = {}
        for dir_name in ["raw", "processed", "output", "floor_plans"]:
            dir_path = self.base_path / dir_name
            if dir_path.exists():
                files = list(dir_path.rglob("*"))
                total_size = sum(f.stat().st_size for f in files if f.is_file())
                stats[dir_name] = {
                    "files": len([f for f in files if f.is_file()]),
                    "dirs": len([f for f in files if f.is_dir()]),
                    "size_mb": round(total_size / (1024 * 1024), 2)
                }

        # å‰µå»ºè¡¨æ ¼
        table = Table(title="æ•¸æ“šæ•´ç†çµ±è¨ˆ")
        table.add_column("ç›®éŒ„", style="cyan")
        table.add_column("æ–‡ä»¶æ•¸", justify="right", style="green")
        table.add_column("å­ç›®éŒ„æ•¸", justify="right", style="blue")
        table.add_column("å¤§å° (MB)", justify="right", style="magenta")

        for dir_name, stat in stats.items():
            table.add_row(
                dir_name,
                str(stat["files"]),
                str(stat["dirs"]),
                f"{stat['size_mb']:.2f}"
            )

        console.print(table)

        # ç”Ÿæˆå ±å‘Šæ–‡ä»¶
        report = {
            "organized_date": self.timestamp,
            "statistics": stats,
            "actions": [
                "å‰µå»ºå‚™ä»½",
                "æ•´ç† raw/ ç›®éŒ„ä¸¦ç”Ÿæˆå…ƒæ•¸æ“š",
                "æ•´ç† processed/ ç›®éŒ„æŒ‰æ ¡å€åˆ†é¡",
                "æ•´ç† output/ ç›®éŒ„ä¸¦å»ºç«‹ç‰ˆæœ¬æ§åˆ¶",
                "æ•´ç† floor_plans/ ç›®éŒ„æŒ‰é¡å‹åˆ†é¡",
                "ç‚ºæ‰€æœ‰ç›®éŒ„å‰µå»º README.md"
            ],
            "next_steps": [
                "é©—è­‰æ–‡ä»¶å®Œæ•´æ€§",
                "å£“ç¸®èˆŠç‰ˆæœ¬æ•¸æ“š",
                "æ›´æ–°ä¸» README.md"
            ]
        }

        report_file = self.base_path / f"organization_report_{self.timestamp}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        console.print(f"\n[green]âœ“ å ±å‘Šå·²ä¿å­˜: {report_file.name}[/green]")

        return str(report_file)


def main():
    """ä¸»ç¨‹åº"""
    console.print("[bold cyan]ğŸš€ NQSD æ•¸æ“šæ•´ç†å·¥å…·[/bold cyan]\n")

    base_path = Path("/data")
    organizer = DataOrganizer(base_path)

    try:
        # 1. å‰µå»ºå‚™ä»½
        organizer.create_backup()

        # 2. æ•´ç†å„ç›®éŒ„
        organizer.organize_raw()
        organizer.organize_processed()
        organizer.organize_output()
        organizer.organize_floor_plans()

        # 3. ç”Ÿæˆå ±å‘Š
        organizer.generate_report()

        console.print("\n[bold green]âœ… æ‰€æœ‰æ•¸æ“šæ•´ç†å®Œæˆï¼[/bold green]")

    except Exception as e:
        console.print(f"\n[bold red]âŒ éŒ¯èª¤: {e}[/bold red]")
        raise


if __name__ == "__main__":
    main()
